[{"content":"1. 为什么做量化 通过改变参数存储的类型（从较大比特的存储方式转化为较小比特的存储方式，变化参数的粒度），实现模型大小的压缩，已达到以下目的：\n减少内存占用：从FP32可以压缩到16位、8位甚至4位（FP32-\u0026gt;INT8） 提高推理速度：一些硬件平台计算INT比FLOAT快得多 降低功耗以及便于模型部署 访问一次32位浮点数的时间可以用来访问四次8位整数\n2. 量化方法 量化粒度：\n逐层量化：以一层网络为量化单位，每层网络一组量化参数。这种方法最简单，但可能会引入较大的精度损失。 逐通道量化：以一层网络的每个量化通道为单位，每个通道单独使用一组量化参数（s,z)。逐通道量化由于量化粒度更细，能获得更高的量化精度，但计算也更复杂。 逐组量化：以组为单位，每个组使用一组量化参数。其粒度处于逐层量化和逐通道量化之间。 量化的两个步骤：前向量化和反向量化\n前向量化：r为量化前的float,q为量化后的int，s是数据量化的间隔，z为bias :\n$q=clip(round(\\frac{r}{s}+z),q_{min},q_{max})$\n$S=\\frac{maxVal-minVal}{quantMax-quantMin}$\n$Z=quantMax-\\frac{maxVal}{s}$\n​\t反向量化：\n​\t$r\u0026rsquo;=s*(q-z)$\n量化方式 QAT PTQ 定义 量化感知训练（Quantization Aware Training, QAT）：将训练过的模型量化后又再进行重训练，由于定点数值无法用于反向梯度计算，实际操作过程是在某些op前插入伪量化节点（fake quantization nodes）， 用于在训练时获取流经该op的数据的截断值，便于在部署量化模型时对节点进行量化时使用。\n核心思想是将量化目标无缝地集成到模型的训练过程中，使模型在训练阶段就能够适应低精度表示，从而减少量化后可能带来的精度损失。 训练后量化（Post-Training Quantization, PTQ）：使用一批校准数据对训练好的模型进行校准，过程中无需对原始模型进行任何训练。 具体流程 模型初始化：设置模型的初始权重和量化参数。伪量化：在训练过程中，对每个前向传播步骤应用伪量化算子，将浮点数转换为整数并再转换回浮点数（即模拟量化过程）。损失计算：计算模型在伪量化后的损失。 参数更新：根据损失函数对模型权重进行更新。模型验证：在验证数据集上评估模型性能，并根据需要调整量化参数。 模型导出：在训练完成后，导出量化后的模型。 数据收集：收集用于校准的代表性数据集。 量化参数确定：根据校准数据集确定量化参数，如缩放因子和偏移量。 模型量化：将模型权重和激活值按照量化参数进行转换。 模型验证：在验证数据集上评估量化后模型的性能，确保精度损失在可接受范围内。 QAT中，在反向传播过程中，因为量化后的权重是离散的，反向传播的时候对 W 求导数为 0，因为梯度为 0，所以网络学习不到任何内容，权重 W也不会更新，这里可以使用直通估计器（Straight-Through Estimator，简称 STE）简单地将梯度通过量化传递：STE 近似假设量化操作的梯度为 1，从而允许梯度直接通过量化节点。\n量化对象：\n仅权重量化（Weight-Only Quantization）相对简单，但性能提升不如权重和激活量化（Weight-and-Activation Quantization），因为实际应用中激活值往往才是内存的大头。\n3. PTQ 3.1 PTQ量化INT8 步骤：\n确定缩放因子：**缩放因子（scale）**是通过浮点数中的最大值和最小值计算得到的，它决定了浮点数到整数的映射关系。\n量化：将浮点数除以缩放因子，四舍五入到最近的整数，并限制在[-128, 127]范围内。\n对称量化：核心是零点的处理，要保证原始零点在量化后依然对应于整数区间的0，映射到[-128,127]；非对称量化映射到[0,255]\n反量化：在推理过程中，需要将量化后的整数重新转换为浮点数，以便进行后续计算。反量化是通过将量化后的整数乘以缩放因子来实现的。\n在实际应用中，除了权重外，还需要对激活值进行量化。激活值的量化通常在推理过程中动态进行，即在每一层计算前将激活值量化为INT8，然后在计算完成后将结果反量化为FP32，以便传递给下一层。\n激活值直接依赖输入数据特征，不同样本会导致激活分布剧烈波动，若采用静态量化，固定scale/zero_point难以覆盖所有输入场景;\n静态量化在模型部署前就通过校准数据集预先计算量化参数（s,z）动态量化在推理过程中实时计算激活值的量化参数，仅对权重进行预量化（延迟更高）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import torch import torch.nn as nn import torch.quantization as quant import torch.optim as optim import torch.nn.functional as F # 定义一个简单的卷积神经网络模型 class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3, 1) self.conv2 = nn.Conv2d(32, 64, 3, 1) self.fc1 = nn.Linear(9216, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2) x = torch.flatten(x, 1) x = F.relu(self.fc1(x)) x = self.fc2(x) return F.log_softmax(x, dim=1) # 初始化模型 model = SimpleCNN() # 准备校准数据集（用于收集统计信息） calibration_dataloader = ... # 这里需要替换为实际的校准数据集加载器 # 模型校准 model.eval() with torch.no_grad(): for inputs, _ in calibration_dataloader: model(inputs) # 准备量化配置 model.qconfig = quant.get_default_qconfig(\u0026#39;fbgemm\u0026#39;) # fbgemm是Facebook开发的量化计算库，支持INT8矩阵乘加速 # 默认配置包含\u0026#39;对称量化\u0026#39;的权重参数和\u0026#39;非对称量化\u0026#39;的激活值参数，采用最小-最大校准策略 # 该配置会为卷积层和全连接层自动添加量化/反量化节点 quant.prepare(model, inplace=True) # 遍历所有子模块，将nn.Conv2d替换为nn.quantized.Conv2d的占位符 # 在激活层输出位置插入HistogramObserver，记录前向传播时的数值分布 # 转换模型为量化模式 model.cpu() quant.convert(model, inplace=True) # 移除所有Observer模块，保留校准得到的scale/zero_point参数 # 将nn.quantized.Conv2d占位符替换为nnq.Conv2d量化实现 # 将权重参数转换为INT8格式 # 保存量化后的模型 torch.save(model.state_dict(), \u0026#39;quantized_model.pth\u0026#39;) 3.2 SmoothQuant(SQ)量化方式 原理：\nSmoothQuant的核心思想是在量化过程中引入平滑性约束，以减少量化误差（信息损失），通过优化一个包含量化误差和平滑性约束的损失函数实现。\n主要步骤：收集统计信息→计算量化参数→平滑量化→模型校准（校准数据集微调）\n​\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 model = SimpleNet() # 准备校准数据集（用于收集统计信息） calibration_dataloader = ... # 这里需要替换为实际的校准数据集加载器 # 收集权重和激活值的统计信息 def collect_stats(model, dataloader): act_max = {} act_min = {} for inputs, _ in dataloader: model(inputs) for name, module in model.named_modules(): if isinstance(module, nn.Linear): weight = module.weight.detach().cpu().numpy() act_max[name + \u0026#39;.weight\u0026#39;] = np.max(weight) act_min[name + \u0026#39;.weight\u0026#39;] = np.min(weight) # 注意：这里省略了激活值的统计，实际中需要添加 return act_max, act_min act_max, act_min = collect_stats(model, calibration_dataloader) # 准备量化配置 model.qconfig = quant.get_default_qconfig(\u0026#39;fbgemm\u0026#39;) quant.prepare(model, inplace=True) # 自定义量化函数，引入平滑性约束-\u0026gt;一个正则项 def smooth_quantize(tensor, scale, zero_point, smooth_factor=0.1): quantized = torch.round(tensor / scale) - zero_point quantized = torch.clamp(quantized, min=-128, max=127).to(tensor.dtype) smoothed = tensor + smooth_factor * (quantized.to(tensor.dtype) * scale + zero_point - tensor) return quantized, smoothed # 对模型进行量化，并引入平滑性约束 def quantize_model(model, act_max, act_min): for name, module in model.named_modules(): if isinstance(module, nn.Linear): weight = module.weight.detach().cpu().numpy() scale = (act_max[name + \u0026#39;.weight\u0026#39;] - act_min[name + \u0026#39;.weight\u0026#39;]) / 255.0 zero_point = 0 # 简化处理，实际中需要计算 quantized_weight, smoothed_weight = smooth_quantize( torch.tensor(weight, dtype=torch.float32), scale, zero_point ) module.weight.data = torch.tensor(smoothed_weight, dtype=torch.float32).to(module.weight.device) # 注意：这里省略了偏置和激活值的量化，实际中需要添加 return model 3.3 GPTQ(Gradient-based Post-Training Quantization)量化方式 GPTQ是基于梯度的PTQ，通过最小化量化引入的输出误差，主要步骤为：收集校准数据→逐层处理→最小化输出误差→更新权重。在逐层处理中，GPTQ对模型的每一层进行独立量化，避免全局优化的复杂度。逐层量化允许对不同层采用不同的量化策略，以最小化量化带来的误差。同时，GPTQ利用Hessian矩阵来估计量化误差，并优化量化参数，Hessian矩阵是二阶导数矩阵，用于描述损失函数相对于模型参数的二阶变化率。\n4. QAT 4.1 LLM-QAT LLM-QAT的基本思想是使用预训练模型自己生成的数据进行知识蒸馏，并在量化权重和激活的同时，对KV cache进行量化。\n在LLM-QAT中，量化包括权重、激活和KV cache的量化。针对权重采用的是per-channel量化，而针对激活和KV cache采用的是per-token量化。在量化过程中，采用的是均匀线性对称量化，量化方法采用minmax方法（而不是lsq*等方法）。采用对称量化的原因是观察到带有GLU（gated linear unit）的模型权重与激活对称，但对于采用GELU的模型并不适用。 采用基于交叉熵的logit distillation方法，通过teacher model指导student model进行训练。 选择合适的微调数据集非常重要。如果QAT数据域太窄或者与原始预训练数据分布存在显著不同，则可能会损害模型的性能。 LSQ(Learned Step Size Quantization) 是一种在模型量化中动态学习量化步长的先进方法,在训练过程中通过梯度下降优化步长，使其适应数据分布，减少精度损失。\n参考链接：\n[llm-deploy](https://github.com/datawhalechina/llm-deploy/tree/main/docs/chapter1)\n一文搞懂神经网络模型量化\n","date":"2025-04-20T00:00:00Z","permalink":"https://Nozom1ff.github.io/p/llm_quantization/","title":"LLM_Quantization"},{"content":"Hello World! ","date":"2019-03-05T00:00:00Z","permalink":"https://Nozom1ff.github.io/p/emoji-support/","title":"Emoji Support"}]